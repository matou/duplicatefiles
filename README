The idea came when I tried to clean up my old backups. I was looking for files
that I will still need and deleting duplicate backups. Since I used to
completely copy all my data to my backup drive every time I wanted to backup I
have a hell of a mess. I don't want to lose any data. Especially not my
pictures. I hope this will speed up cleaning. The goal is to identify duplicates
in my backups and thusly reduce the data that I have to check manually.

Since the script now uses a database for its collected data instead of having
everything in memory it should work even with huge amounts of files.

Once the tests with my hard drives were successfull I will start to cleaning up
the code a bit.

License
-------

Copyright (c) 2011 Matthias Matousek <matou@taunusstein.net>

Permission to use, copy, modify, and distribute this software for any
purpose with or without fee is hereby granted, provided that the above
copyright notice and this permission notice appear in all copies.

THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES
WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF
MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR
ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN
ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF
OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
